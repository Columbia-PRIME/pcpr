<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Theory crash course • pcpr</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><script src="../deps/MathJax-3.2.2/tex-chtml.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Theory crash course">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">pcpr</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/pcp-applied.html">Air pollution source apportionment with PCP</a></li>
    <li><a class="dropdown-item" href="../articles/pcp-quickstart.html">Quickstart: applying PCP to a simulated environmental mixture</a></li>
    <li><a class="dropdown-item" href="../articles/theory-crash-course.html">Theory crash course</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/Columbia-PRIME/pcpr/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>Theory crash course</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/Columbia-PRIME/pcpr/blob/v1.0.0/vignettes/theory-crash-course.Rmd" class="external-link"><code>vignettes/theory-crash-course.Rmd</code></a></small>
      <div class="d-none name"><code>theory-crash-course.Rmd</code></div>
    </div>

    
    
<p>The R package <code>pcpr</code> implements Principal Component
Pursuit (PCP), a robust dimensionality reduction technique, for pattern
recognition tailored to environmental health (EH) data. The statistical
methodology and computational details are provided in Gibson et
al. (2022).</p>
<p>In this code-free vignette, we present a crash course in PCP’s
theoretical background, so researchers can better navigate all of the
functionality offered in <code>pcpr</code>. We will touch upon:</p>
<ol style="list-style-type: decimal">
<li>PCP’s modeling overview</li>
<li>The low-rank and sparse matrices <code>L</code> and
<code>S</code>
</li>
<li>The EH-specific extensions in <code>pcpr</code>
</li>
<li>Convex and non-convex PCP</li>
<li>PCP parameters <code>lambda</code>, <code>mu</code>, and
<code>eta</code>
</li>
<li>Tuning parameters with <code><a href="../reference/grid_search_cv.html">grid_search_cv()</a></code>
</li>
</ol>
<p>We recommend users skim this crash course before reading the two
code-heavy vignettes:</p>
<ul>
<li>
<code><a href="../articles/pcp-quickstart.html">vignette("pcp-quickstart")</a></code>: applying PCP to a simulated
environmental mixture</li>
<li>
<code><a href="../articles/pcp-applied.html">vignette("pcp-applied")</a></code>: employing PCP for source
apportionment of real-world PM2.5 air pollution concentration data using
the <code>queens</code> dataset</li>
</ul>
<div class="section level2">
<h2 id="pcp-modeling-overview">PCP modeling overview<a class="anchor" aria-label="anchor" href="#pcp-modeling-overview"></a>
</h2>
<p>PCP algorithms model an observed exposure matrix <span class="math inline">\(D\)</span> as the sum of three underlying
ground-truth matrices:</p>
<p><img src="imgs%2Fpcp-model-1.jpeg" width="100%"></p>
<p>a low-rank matrix <span class="math inline">\(L_0\)</span> encoding
consistent patterns of exposure, a sparse matrix <span class="math inline">\(S_0\)</span> isolating unique or outlying exposure
events (that cannot be explained by the consistent exposure patterns),
and dense noise <span class="math inline">\(Z_0\)</span>. All of these
matrices are of dimension <span class="math inline">\(n \times
p\)</span>, where <span class="math inline">\(n\)</span> is the number
of observations (e.g. study participants or measurement dates) and <span class="math inline">\(p\)</span> is the number of exposures
(e.g. chemical and/or non-chemical stressors). Beyond this mixtures
model, the main assumption made by PCP is that <span class="math inline">\(Z_0 \sim N(\mu, \sigma^2)\)</span> consists of
independently and identically distributed (i.i.d.) Gaussian noise
corrupting each entry of the overall exposure matrix <span class="math inline">\(D\)</span>.</p>
<p>The models in <code>pcpr</code> seek to decompose an observed data
matrix <span class="math inline">\(D\)</span> into estimated low-rank
and sparse components <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span> for use in downstream environmental
health analyses.</p>
</div>
<div class="section level2">
<h2 id="the-low-rank-matrix">The low-rank matrix<a class="anchor" aria-label="anchor" href="#the-low-rank-matrix"></a>
</h2>
<p>The estimated low-rank matrix <span class="math inline">\(L\)</span>
provides information on the consistent exposure patterns,
satisfying:</p>
<p><span class="math display">\[r = \text{rank}(L) \ll \min(n,
p).\]</span></p>
<p>The rank <span class="math inline">\(r\)</span> of a matrix is the
number of linearly independent columns or rows in the matrix, and plays
an important role in defining the mathematical structure of the data.
Intuitively, the rank directly corresponds to the (relatively few)
number of underlying patterns governing the mixture. Here, “patterns”
can refer to specific sources, profiles or behaviors leading to
exposure, depending on the application.</p>
<p>Contrary to closely related dimension reduction tools such as
principal component analysis (PCA), PCP infers the rank <span class="math inline">\(r\)</span> from the observed data. In
<code>pcpr</code>, this is done directly during optimization for convex
PCP, and via grid search for non-convex PCP. As such, rather than
require the researcher to choose the number of estimated patterns for
use in subsequent health models, PCP allows the observed data to “speak
for itself”, thereby removing potential points of subjectivity in model
design.</p>
<p>Notice that <span class="math inline">\(L \in \mathbb{R}^{n \times
p}\)</span>, meaning it is still defined in terms of the original <span class="math inline">\(n\)</span>-many observations and <span class="math inline">\(p\)</span>-many environmental variables. Put
differently, <span class="math inline">\(L\)</span> can be taken as a
robust approximation to the true environmental mixture matrix,
unperturbed by outliers (captured in <span class="math inline">\(S\)</span>) or noise (handled by PCP’s noise
term). In this way, the latent exposure patterns are <em>encoded</em> in
<span class="math inline">\(L\)</span> rather than directly estimated.
To <em>explicitly</em> obtain the exposure patterns from <span class="math inline">\(L\)</span>, PCP may then be paired with various
matrix factorization methods (e.g., PCA, factor analysis, or
non-negative matrix factorization) that yield chemical loadings and
individual scores for use in downstream health models.</p>
<p>This flexibility allows <span class="math inline">\(L\)</span> to
adapt to mixture-specific assumptions. For example, if the assumption of
orthogonal (i.e., independent) patterns is too strong, then instead of
pairing <span class="math inline">\(L\)</span> with PCA, a more
appropriate method such as factor analysis can be used. Alternatively,
depending on the sample size and study design, <span class="math inline">\(L\)</span> may also be directly incorporated into
regression models.</p>
</div>
<div class="section level2">
<h2 id="the-sparse-matrix">The sparse matrix<a class="anchor" aria-label="anchor" href="#the-sparse-matrix"></a>
</h2>
<p>The estimated sparse matrix <span class="math inline">\(S\)</span>
captures unusually high or low outlying exposure events, unexplained by
the identified patterns in <span class="math inline">\(L\)</span>. Most
entries in <span class="math inline">\(S\)</span> are 0, with non-zero
entries identifying such extreme exposure activity. The number, location
(i.e., support), and value of non-zero entries in <span class="math inline">\(S\)</span> need not be a priori defined; PCP
isolates these itself during optimization.</p>
<p>By separating and retaining sparse exposure events, PCP boasts an
enormous advantage over other current dimension reduction techniques.
Despite being common phenomena in mixtures data, sparse outliers are
typically removed from the exposure matrix prior to analysis. This is
because PCA and other conventional dimension reduction approaches are
unable to disentangle such unique events from the overall patterns of
exposure: If included, even low fractions of outliers can deviate
patterns identified by traditional methods away from the true
distribution of the data, yielding inaccurate pattern estimations and
high false positive rates of detected outliers. By decomposing a mixture
into low-rank and sparse components <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span>, PCP avoids such pitfalls.</p>
</div>
<div class="section level2">
<h2 id="extensions-for-environmental-health-data">Extensions for environmental health data<a class="anchor" aria-label="anchor" href="#extensions-for-environmental-health-data"></a>
</h2>
<p>The functions in <code>pcpr</code> are outfitted with three
environmental health (EH)-specific extensions, making <code>pcpr</code>
particularly powerful for EH research:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Missing value functionality:</strong> PCP is able to recover
<code>NA</code> values in the observed mixture matrix, often
outperforming traditional imputation techniques.</li>
<li>
<strong>Leveraging potential limit of detection (LOD)
information:</strong> When equipped with LOD information, PCP treats any
estimations of values known to be below the LOD as equally valid if
their approximations fall between 0 and the LOD. PCP with LOD data often
outperforms PCA imputed with <span class="math inline">\(\frac{LOD}{\sqrt{2}}\)</span>.</li>
<li>
<strong>Non-negativity constraint on the estimated <code>L</code>
matrix:</strong> If desired, PCP can enforce values in the estimated
low-rank matrix <code>L</code> to be <span class="math inline">\(\geq
0\)</span>, better modeling real world mixtures data.</li>
</ol>
<div class="section level3">
<h3 id="missing-value-functionality">Missing value functionality<a class="anchor" aria-label="anchor" href="#missing-value-functionality"></a>
</h3>
<p>PCP assumes that the same data generating mechanisms govern both the
missing and the observed entries in <span class="math inline">\(D\)</span>. Because PCP primarily seeks accurate
estimation of <em>patterns</em> rather than individual
<em>observations</em>, this assumption is reasonable, but in some edge
cases may not always be justified. Missing values in <span class="math inline">\(D\)</span> are therefore reconstructed in the
recovered low-rank <span class="math inline">\(L\)</span> matrix
according to the underlying patterns in <span class="math inline">\(L\)</span>. There are three corollaries to keep in
mind regarding the quality of recovered missing observations:</p>
<ol style="list-style-type: decimal">
<li>Recovery of missing entries in <span class="math inline">\(D\)</span> relies on accurate estimation of <span class="math inline">\(L\)</span>;</li>
<li>The fewer observations there are in <span class="math inline">\(D\)</span>, the harder it is to accurately
reconstruct <span class="math inline">\(L\)</span> (therefore estimation
of <em>both</em> unobserved <em>and</em> observed measurements in <span class="math inline">\(L\)</span> degrades); and</li>
<li>Greater proportions of missingness in <span class="math inline">\(D\)</span> artificially drive up the sparsity of
the estimated <span class="math inline">\(S\)</span> matrix. This is
because it is not possible to recover a sparse event in <span class="math inline">\(S\)</span> when the corresponding entry in <span class="math inline">\(D\)</span> is unobserved. By definition, sparse
events in <span class="math inline">\(S\)</span> cannot be explained by
the consistent patterns in <span class="math inline">\(L\)</span>.
Practically, if 20% of the entries in <span class="math inline">\(D\)</span> are missing, then at least 20% of the
entries in <span class="math inline">\(S\)</span> will be 0.</li>
</ol>
</div>
<div class="section level3">
<h3 id="leveraging-potential-limit-of-detection-lod-information">Leveraging potential limit of detection (LOD) information<a class="anchor" aria-label="anchor" href="#leveraging-potential-limit-of-detection-lod-information"></a>
</h3>
<p>When equipped with <span class="math inline">\(LOD\)</span>
information, PCP treats any estimations of values known to be below the
<span class="math inline">\(LOD\)</span> as equally valid if their
approximations fall between <span class="math inline">\(0\)</span> and
the <span class="math inline">\(LOD\)</span>. Over the course of
optimization, observations below the LOD are pushed into this known
range <span class="math inline">\([0, LOD]\)</span> using penalties from
above and below: should a <span class="math inline">\(&lt; LOD\)</span>
estimate be <span class="math inline">\(&lt; 0\)</span>, it is
stringently penalized, since measured observations cannot be negative.
On the other hand, if a <span class="math inline">\(&lt; LOD\)</span>
estimate is <span class="math inline">\(&gt; LOD\)</span>, it is also
heavily penalized: less so than when <span class="math inline">\(&lt;
0\)</span>, but more so than observations known to be above the <span class="math inline">\(LOD\)</span>, because we have prior information
that these observations must be below <span class="math inline">\(LOD\)</span>. Observations known to be above the
<span class="math inline">\(LOD\)</span> are penalized as usual, using
the Frobenius norm in the above objective function.</p>
<p>Gibson et al. (2022) demonstrate that in experimental settings with
up to 50% of the data corrupted below the <span class="math inline">\(LOD\)</span>, PCP with the <span class="math inline">\(LOD\)</span> extension boasts superior accuracy of
recovered <span class="math inline">\(L\)</span> models compared to PCA
coupled with <span class="math inline">\(\frac{LOD}{\sqrt{2}}\)</span>
imputation. PCP even outperforms PCA in low-noise scenarios with as much
as 75% of the data corrupted below the <span class="math inline">\(LOD\)</span>. The few situations in which PCA
bettered PCP were those pathological cases in which <span class="math inline">\(D\)</span> was characterized by extreme noise and
huge proportions (i.e., 75%) of observations falling below the <span class="math inline">\(LOD\)</span>.</p>
</div>
<div class="section level3">
<h3 id="non-negativity-constraint-on-the-estimated-l-matrix">Non-negativity constraint on the estimated <code>L</code>
matrix<a class="anchor" aria-label="anchor" href="#non-negativity-constraint-on-the-estimated-l-matrix"></a>
</h3>
<p>To enhance interpretability of PCP-rendered solutions, there is an
optional non-negativity constraint that can be imposed on the <span class="math inline">\(L\)</span> matrix to ensure all estimated values
within it are <span class="math inline">\(\geq 0\)</span>. This prevents
researchers from having to deal with negative observation values and
questions surrounding their meaning and utility. Non-negative <span class="math inline">\(L\)</span> models also allow for seamless use of
methods such as non-negative matrix factorization to extract
non-negative patterns.</p>
<p>Currently, the non-negativity constraint is only supported in the
convex PCP function <code><a href="../reference/root_pcp.html">root_pcp()</a></code>, incorporated in the ADMM
splitting technique via the introduction of an additional optimization
variable and corresponding constraint. Future work will extend the
constraint to the non-convex PCP method <code><a href="../reference/rrmc.html">rrmc()</a></code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="convex-vs--non-convex-pcp">Convex vs. non-convex PCP<a class="anchor" aria-label="anchor" href="#convex-vs--non-convex-pcp"></a>
</h2>
<p>Of the many flavors of PCP undergoing active study in the current
literature, we provide two distinct models in <code>pcpr</code>: the
convex model <code><a href="../reference/root_pcp.html">root_pcp()</a></code> and non-convex model
<code><a href="../reference/rrmc.html">rrmc()</a></code>. The table below offers a quick glance at their
relative differences:</p>
<table class="table">
<colgroup>
<col width="43%">
<col width="29%">
<col width="27%">
</colgroup>
<thead><tr class="header">
<th></th>
<th><code><a href="../reference/root_pcp.html">root_pcp()</a></code></th>
<th><code><a href="../reference/rrmc.html">rrmc()</a></code></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Convex?</td>
<td><em>Yes</em></td>
<td><em>No</em></td>
</tr>
<tr class="even">
<td>Convergence?</td>
<td><em>Slow</em></td>
<td><em>Fast</em></td>
</tr>
<tr class="odd">
<td>Expected low-rank structure?</td>
<td><em>Well-defined</em></td>
<td><em>Complex</em></td>
</tr>
<tr class="even">
<td>Parameters?</td>
<td><span class="math inline">\(D, \lambda, \mu\)</span></td>
<td><span class="math inline">\(D, r, \eta\)</span></td>
</tr>
<tr class="odd">
<td>Supports missing values?</td>
<td><em>Yes</em></td>
<td><em>Yes</em></td>
</tr>
<tr class="even">
<td>Supports LOD penalty?</td>
<td><em>Yes</em></td>
<td><em>Yes</em></td>
</tr>
<tr class="odd">
<td>Supports non-negativity constraint?</td>
<td><em>Yes</em></td>
<td><em>No</em></td>
</tr>
<tr class="even">
<td>Rank determination?</td>
<td><em>Autonomous</em></td>
<td>
<em>User-defined</em>*</td>
</tr>
<tr class="odd">
<td>Sparse event identification?</td>
<td><em>Autonomous</em></td>
<td><em>Autonomous</em></td>
</tr>
<tr class="even">
<td>Optimization approach?</td>
<td><em>ADMM</em></td>
<td><em>Iterative rank-based</em></td>
</tr>
</tbody>
</table>
<p>*<code><a href="../reference/rrmc.html">rrmc()</a></code> can be paired with the cross-validated
<code><a href="../reference/grid_search_cv.html">grid_search_cv()</a></code> function for autonomous rank
determination.</p>
<p>Convex PCP via <code><a href="../reference/root_pcp.html">root_pcp()</a></code> is best for data characterized
by rapidly decaying singular values (e.g. image and video data),
indicative of very well-defined latent patterns.</p>
<p>Non-convex PCP with <code><a href="../reference/rrmc.html">rrmc()</a></code> is best suited for data
characterized by slowly decaying singular values, indicative of complex
underlying patterns and a relatively large degree of noise. Most EH data
can be described this way, so we expect most EH researchers to utilize
<code><a href="../reference/rrmc.html">rrmc()</a></code> in their analyses, however there are cases where the
convexity of <code><a href="../reference/root_pcp.html">root_pcp()</a></code> may be preferable.</p>
<div class="section level3">
<h3 id="convex-pcp">Convex PCP<a class="anchor" aria-label="anchor" href="#convex-pcp"></a>
</h3>
<p>Convex PCP formulations possess a number of particularly attractive
properties, foremost of which is convexity, meaning that every local
optimum is a global optimum, and a single best solution exists. Convex
approaches to PCP also have the virtue that the rank <span class="math inline">\(r\)</span> of the recovered <span class="math inline">\(L\)</span> matrix is determined during
optimization, without researcher input.</p>
<p>Unfortunately, these benefits come at a cost: convex PCP programs are
expensive to run on large datasets, suffering from poor convergence
rates. Moreover, convex PCP approaches are best suited to instances in
which the target low-rank matrix <span class="math inline">\(L_0\)</span> can be accurately modelled as
low-rank (i.e. <span class="math inline">\(L_0\)</span> is governed by
only a few very well-defined patterns). This is often the case with
image and video data (characterized by rapidly decaying singular
values), but not common for EH data. EH data is typically only
approximately low-rank (characterized by complex patterns and slowly
decaying singular values).</p>
<p>The convex model available in <code>pcpr</code> is
<code><a href="../reference/root_pcp.html">root_pcp()</a></code>. For a comprehensive technical understanding, we
refer readers to <a href="https://proceedings.neurips.cc/paper/2021/file/f65854da4622c1f1ad4ffeb361d7703c-Paper.pdf" class="external-link">Zhang
et al. (2021)</a> introducing the algorithm.</p>
<p><code><a href="../reference/root_pcp.html">root_pcp()</a></code> optimizes the following objective
function:</p>
<p><span class="math display">\[\min_{L, S} ||L||_* + \lambda ||S||_1 +
\mu ||L + S - D||_F\]</span></p>
<p>The first term is the nuclear norm of the L matrix, incentivizing
<span class="math inline">\(L\)</span> to be low-rank. The second term
is the <span class="math inline">\(\ell_1\)</span> norm of the S matrix,
encouraging S to be sparse. The third term is the Frobenius norm applied
to the model’s noise, ensuring that the estimated low-rank and sparse
models <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span> together have high fidelity to the
observed data <span class="math inline">\(D\)</span>. The objective is
not smooth nor differentiable, however it is convex and separable. As
such, it is optimized using the Alternating Direction Method of
Multipliers (ADMM) algorithm (Boyd et al. (2011)), (Gao et
al. (2020)).</p>
</div>
<div class="section level3">
<h3 id="non-convex-pcp">Non-convex PCP<a class="anchor" aria-label="anchor" href="#non-convex-pcp"></a>
</h3>
<p>To alleviate the high computational complexity of convex methods,
non-convex PCP frameworks have been developed. These drastically improve
upon the convergence rates of their convex counterparts. Better still,
non-convex PCP methods more flexibly accommodate data lacking a
well-defined low-rank structure, so they are from the outset better
suited to handling EH data. Non-convex formulations provide this
flexibility by allowing the user to interrogate the data at different
ranks.</p>
<p>The drawback here is that non-convex algorithms can no longer
determine the rank best describing the data on their own, instead
requiring the researcher to subjectively specify the rank <span class="math inline">\(r\)</span> as in PCA. However, by pairing
non-convex PCP algorithms with the cross-validation routine implemented
in the <code><a href="../reference/grid_search_cv.html">grid_search_cv()</a></code> function, the optimal rank can be
determined semi-autonomously; the researcher need only define a rank
<em>search space</em> from which the <em>optimal rank will be identified
via grid search</em>. One of the more glaring trade-offs made by
non-convex methods for this improved run-time and flexibility is weaker
theoretical promises; specifically, non-convex PCP runs the risk of
finding spurious <em>local</em> optima, rather than the <em>global</em>
optimum guaranteed by their convex siblings. Having said that, theory
has been developed guaranteeing equivalent performance between
non-convex implementations and closely related convex formulations under
certain conditions. These advancements provide strong motivation for
non-convex frameworks despite their weaker theoretical promises.</p>
<p>The non-convex model available in <code>pcpr</code> is
<code><a href="../reference/rrmc.html">rrmc()</a></code>. We refer readers to <a href="https://proceedings.mlr.press/v70/cherapanamjeri17a.html" class="external-link">Cherapanamjeri
et al. (2017)</a> for an in depth look at the mathematical details.</p>
<p><code><a href="../reference/rrmc.html">rrmc()</a></code> implicitly optimizes the following objective
function:</p>
<p><span class="math display">\[\min_{L, S} I_{rank(L) \leq r} + \eta
||S||_0 + ||L + S - D||_F^2\]</span></p>
<p>The first term is the indicator function checking that the <span class="math inline">\(L\)</span> matrix is strictly rank <span class="math inline">\(r\)</span> or less, implemented using a rank <span class="math inline">\(r\)</span> projection operator
<code><a href="../reference/proj_rank_r.html">proj_rank_r()</a></code>. The second term is the <span class="math inline">\(\ell_0\)</span> norm applied to the <span class="math inline">\(S\)</span> matrix to encourage sparsity, and is
implemented with the help of an adaptive hard-thresholding operator
<code><a href="../reference/hard_threshold.html">hard_threshold()</a></code>. The third term is the squared Frobenius
norm applied to the model’s noise.</p>
<p><code><a href="../reference/rrmc.html">rrmc()</a></code> uses an incremental rank-based strategy in order
to estimate <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span>: First, a rank-<span class="math inline">\(1\)</span> model <span class="math inline">\((L^{(1)}, S^{(1)})\)</span> is estimated. The
rank-<span class="math inline">\(1\)</span> model is then used as an
initialization point to construct a rank-<span class="math inline">\(2\)</span> model <span class="math inline">\((L^{(2)}, S^{(2)})\)</span>, and so on, until the
desired rank-r model <span class="math inline">\((L^{(r)},
S^{(r)})\)</span> is recovered. All models from ranks <span class="math inline">\(1\)</span> through <span class="math inline">\(r\)</span> are returned by <code><a href="../reference/rrmc.html">rrmc()</a></code> in
this way.</p>
</div>
</div>
<div class="section level2">
<h2 id="pcp-parameters">PCP parameters<a class="anchor" aria-label="anchor" href="#pcp-parameters"></a>
</h2>
<div class="section level3">
<h3 id="intuition-behind-lambda-mu-and-eta">Intuition behind <code>lambda</code>, <code>mu</code>, and
<code>eta</code><a class="anchor" aria-label="anchor" href="#intuition-behind-lambda-mu-and-eta"></a>
</h3>
<p>Recall <code><a href="../reference/root_pcp.html">root_pcp()</a></code>’s objective function is given by:</p>
<p><span class="math display">\[\min_{L, S} ||L||_* + \lambda ||S||_1 +
\mu ||L + S - D||_F\]</span></p>
<ul>
<li>
<span class="math inline">\(\lambda\)</span> (<code>lambda</code>)
controls the sparsity of <code><a href="../reference/root_pcp.html">root_pcp()</a></code>’s output <span class="math inline">\(S\)</span> matrix; larger values of <span class="math inline">\(\lambda\)</span> penalize non-zero entries in
<span class="math inline">\(S\)</span> more stringently, driving the
recovery of sparser <span class="math inline">\(S\)</span> matrices.
Therefore, if you a priori expect few outlying events in your model, you
might expect a grid search to recover relatively larger <span class="math inline">\(\lambda\)</span> values, and vice-versa.</li>
<li>
<span class="math inline">\(\mu\)</span> (<code>mu</code>) adjusts
<code><a href="../reference/root_pcp.html">root_pcp()</a></code>’s sensitivity to noise; larger values of <span class="math inline">\(\mu\)</span> penalize errors between the predicted
model and the observed data (i.e. noise), more severely. Environmental
data subject to higher noise levels therefore require a
<code><a href="../reference/root_pcp.html">root_pcp()</a></code> model equipped with smaller mu values (since
higher noise means a greater discrepancy between the observed mixture
and the true underlying low-rank and sparse model). In virtually
noise-free settings (e.g. simulations), larger values of <span class="math inline">\(\mu\)</span> would be appropriate.</li>
</ul>
<p><code><a href="../reference/rrmc.html">rrmc()</a></code>’s objective function is given by:</p>
<p><span class="math display">\[\min_{L, S} I_{rank(L) \leq r} + \eta
||S||_0 + ||L + S - D||_F^2\]</span></p>
<ul>
<li>
<span class="math inline">\(\eta\)</span> (<code>eta</code>)
controls the sparsity of <code><a href="../reference/rrmc.html">rrmc()</a></code>’s output <span class="math inline">\(S\)</span> matrix, just as <span class="math inline">\(\lambda\)</span> does for <code><a href="../reference/root_pcp.html">root_pcp()</a></code>.
Because there are no other parameters scaling the noise term, <span class="math inline">\(\eta\)</span> can be thought of as a ratio between
<code><a href="../reference/root_pcp.html">root_pcp()</a></code>’s <span class="math inline">\(\lambda\)</span>
and <span class="math inline">\(\mu\)</span>: Larger values of <span class="math inline">\(\eta\)</span> will place a greater emphasis on
penalizing the non-zero entries in <span class="math inline">\(S\)</span> over penalizing the errors between the
predicted and observed data (the dense noise <span class="math inline">\(Z\)</span>).</li>
</ul>
</div>
<div class="section level3">
<h3 id="theoretically-optimal-parameters">Theoretically optimal parameters<a class="anchor" aria-label="anchor" href="#theoretically-optimal-parameters"></a>
</h3>
<p>The <code><a href="../reference/get_pcp_defaults.html">get_pcp_defaults()</a></code> function calculates the “default”
PCP parameter settings of <span class="math inline">\(\lambda\)</span>,
<span class="math inline">\(\mu\)</span> and <span class="math inline">\(\eta\)</span> for a given data matrix <span class="math inline">\(D\)</span>.</p>
<p>The “default” values of <span class="math inline">\(\lambda\)</span>
and <span class="math inline">\(\mu\)</span> offer <em>theoretical</em>
guarantees of optimal estimation performance. Candès et al. (2011)
obtained the guarantee for <span class="math inline">\(\lambda\)</span>,
while Zhang et al. (2021) obtained the result for <span class="math inline">\(\mu\)</span>. It has not yet been proven whether
or not <span class="math inline">\(\eta\)</span> enjoys similar
properties.</p>
<p>The theoretically optimal <span class="math inline">\(\lambda_*\)</span> is given by:</p>
<p><span class="math display">\[\lambda_* = 1 / \sqrt{\max(n,
p)},\]</span></p>
<p>where <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> are the dimensions of the input matrix
<span class="math inline">\(D_{n \times p}\)</span>.</p>
<p>The theoretically optimal <span class="math inline">\(\mu_*\)</span>
is given by: <span class="math display">\[\mu_* = \sqrt{\frac{\min(n,
p)}{2}}.\]</span></p>
<p>The “default” value of <span class="math inline">\(\eta\)</span> is
then simply <span class="math inline">\(\eta =
\frac{\lambda_*}{\mu_*}\)</span>.</p>
</div>
<div class="section level3">
<h3 id="empirically-optimal-parameters">Empirically optimal parameters<a class="anchor" aria-label="anchor" href="#empirically-optimal-parameters"></a>
</h3>
<p>Mixtures data is rarely so well-behaved in practice, however.
Instead, it is common to find different <em>empirically optimal</em>
parameter values after <strong>tuning these parameters in a grid
search</strong>. Therefore, it is recommended to use
<code><a href="../reference/get_pcp_defaults.html">get_pcp_defaults()</a></code> primarily to help define a reasonable
initial parameter search space to pass into
<code><a href="../reference/grid_search_cv.html">grid_search_cv()</a></code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="tuning-parameters-with-grid_search_cv">Tuning parameters with <code>grid_search_cv()</code><a class="anchor" aria-label="anchor" href="#tuning-parameters-with-grid_search_cv"></a>
</h2>
<div class="section level3">
<h3 id="cross-validation-procedure">Cross-validation procedure<a class="anchor" aria-label="anchor" href="#cross-validation-procedure"></a>
</h3>
<p><code><a href="../reference/grid_search_cv.html">grid_search_cv()</a></code> conducts a Monte Carlo style
cross-validated grid search of PCP parameters for a given data matrix
<span class="math inline">\(D\)</span>, PCP function
<code>pcp_fn</code>, and grid of parameter settings to search through
<code>grid</code>. The run time of the grid search can be sped up using
bespoke parallelization settings.</p>
<p>Each hyperparameter setting is cross-validated by:</p>
<ol style="list-style-type: decimal">
<li>Randomly corrupting <span class="math inline">\(\xi\)</span> percent
of the entries in <span class="math inline">\(D\)</span> as missing
(i.e. <code>NA</code> values), yielding <span class="math inline">\(P_\Omega(D)\)</span>. Done using the
<code><a href="../reference/sim_na.html">sim_na()</a></code> function.</li>
<li>Running the given PCP function <code>pcp_fn</code> on <span class="math inline">\(P_\Omega(D)\)</span>, yielding estimates <span class="math inline">\(L\)</span> and <span class="math inline">\(S\)</span>.</li>
<li>Recording the relative recovery error of <span class="math inline">\(L\)</span> compared with the input data matrix
<span class="math inline">\(D\)</span> for only those values that were
imputed as missing during the corruption step (step 1 above). Formally,
the relative error is calculated with: <span class="math display">\[RelativeError(L | D) := \frac{||P_{\Omega^c}(D -
L)||_F}{||P_{\Omega^c}(D)||_F}\]</span>
</li>
<li>Re-running steps 1-3 for a total of <span class="math inline">\(K\)</span>-many runs (each “run” has a unique
random seed from 1 to <span class="math inline">\(K\)</span> associated
with it).</li>
<li>Performance statistics can then be calculated for each “run”, and
then summarized across all runs using mean-aggregated statistics.</li>
</ol>
<p>In the <code><a href="../reference/grid_search_cv.html">grid_search_cv()</a></code> function, <span class="math inline">\(\xi\)</span> is referred to as
<code>perc_test</code> (percent test), while <span class="math inline">\(K\)</span> is known as <code>num_runs</code>
(number of runs).</p>
</div>
<div class="section level3">
<h3 id="best-practices-for-xi-and-k">Best practices for <span class="math inline">\(\xi\)</span> and
<span class="math inline">\(K\)</span><a class="anchor" aria-label="anchor" href="#best-practices-for-xi-and-k"></a>
</h3>
<p>Experimentally, this grid search procedure retrieves the best
performing PCP parameter settings when <span class="math inline">\(\xi\)</span> is relatively low, e.g. <span class="math inline">\(\xi = 0.05\)</span>, or 5%, and <span class="math inline">\(K\)</span> is relatively high, e.g. <span class="math inline">\(K = 100\)</span>. This is because:</p>
<ul>
<li>The larger <span class="math inline">\(\xi\)</span> is, the more the
test set turns into a matrix completion problem, rather than the desired
matrix decomposition problem. To better resemble the actual problem PCP
will be faced with come inference time, <span class="math inline">\(\xi\)</span> should therefore be kept relatively
low.</li>
<li>Choosing a reasonable value for <span class="math inline">\(K\)</span> is dependent on the need to keep <span class="math inline">\(\xi\)</span> relatively low. Ideally, a large
enough <span class="math inline">\(K\)</span> is used so that many (if
not all) of the entries in <span class="math inline">\(D\)</span> are
likely to eventually be tested. Note that since test set entries are
chosen randomly for all runs <span class="math inline">\(1\)</span>
through <span class="math inline">\(K\)</span>, in the pathologically
worst case scenario, the same exact test set could be drawn each time.
In the best case scenario, a different test set is obtained each run,
providing balanced coverage of <span class="math inline">\(D\)</span>.
Viewed another way, the smaller <span class="math inline">\(K\)</span>
is, the more the results are susceptible to overfitting to the
relatively few selected test sets.</li>
</ul>
</div>
<div class="section level3">
<h3 id="interpretation-of-results">Interpretation of results<a class="anchor" aria-label="anchor" href="#interpretation-of-results"></a>
</h3>
<p>Once the grid search of has been conducted, the optimal
hyperparameters can be chosen by examining the output statistics
<code>summary_stats</code>. Below are a few suggestions for how to
interpret the <code>summary_stats</code> table:</p>
<ul>
<li>Generally speaking, the first thing a user will want to inspect is
the <code>rel_err</code> statistic, capturing the relative discrepancy
between recovered test sets and their original, observed (yet possibly
noisy) values. Lower <code>rel_err</code> means the PCP model was better
able to recover the held-out test set. So, in general, the best
parameter settings are those with the lowest <code>rel_err</code>.
Having said this, it is important to remember that this statistic should
be taken with a grain of salt: Because in practice the researcher does
not have access to the ground truth <span class="math inline">\(L\)</span> matrix, the <code>rel_err</code>
measurement is forced to rely on the comparison between the noisy
observed data matrix <span class="math inline">\(D\)</span> and the
estimated low-rank model <span class="math inline">\(L\)</span>. So the
<code>rel_err</code> metric is an “apples to oranges” relative error.
For data that is a priori expected to be subject to a high degree of
noise, it may actually be better to discard parameter settings with
suspiciously low rel_errs (in which case the solution may be
hallucinating an inaccurate low-rank structure from the observed
noise).</li>
<li>For grid searches using <code><a href="../reference/root_pcp.html">root_pcp()</a></code> as the PCP model,
parameters that fail to converge can be discarded. Generally, fewer
<code><a href="../reference/root_pcp.html">root_pcp()</a></code> iterations (<code>num_iter</code>) taken to
reach convergence portend a more reliable / stable solution. In rare
cases, the user may need to increase <code><a href="../reference/root_pcp.html">root_pcp()</a></code>’s
<code>max_iter</code> argument to reach convergence. <code><a href="../reference/rrmc.html">rrmc()</a></code>
does not report convergence metadata, as its optimization scheme runs
for a fixed number of iterations.</li>
<li>Parameter settings with unreasonable sparsity or rank measurements
can also be discarded. Here, “unreasonable” means these reported metrics
flagrantly contradict prior assumptions, knowledge, or work. For
instance, most air pollution datasets contain a number of extreme
exposure events, so PCP solutions returning sparse <span class="math inline">\(S\)</span> models with 100% sparsity have
obviously been regularized too heavily. Note that reported sparsity and
rank measurements are <em>estimates</em> heavily dependent on the
<code>thresh</code> set by the <code><a href="../reference/sparsity.html">sparsity()</a></code> &amp;
<code><a href="../reference/matrix_rank.html">matrix_rank()</a></code> functions. E.g. it could be that the actual
average matrix rank is much higher or lower when a threshold that better
takes into account the relative scale of the singular values is used.
Likewise for the sparsity estimations. Also, recall that the given value
for <span class="math inline">\(\xi\)</span> artifically sets a sparsity
floor, since those missing entries in the test set cannot be recovered
in the <span class="math inline">\(S\)</span> matrix. E.g. if <span class="math inline">\(\xi = 0.05\)</span>, then no parameter setting
will have an estimated sparsity lower than 5%.</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="coded-example-analyses">Coded example analyses<a class="anchor" aria-label="anchor" href="#coded-example-analyses"></a>
</h2>
<p>To see how to apply all of the above in <code>pcpr</code>, we
recommend reading:</p>
<ul>
<li>
<code><a href="../articles/pcp-quickstart.html">vignette("pcp-quickstart")</a></code>: applying PCP to a simulated
environmental mixture</li>
<li>
<code><a href="../articles/pcp-applied.html">vignette("pcp-applied")</a></code>: employing PCP for source
apportionment of real-world PM2.5 air pollution concentration data using
the <code>queens</code> dataset</li>
</ul>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Gibson, Elizabeth A., Junhui Zhang, Jingkai Yan, Lawrence Chillrud,
Jaime Benavides, Yanelli Nunez, Julie B. Herbstman, Jeff Goldsmith, John
Wright, and Marianthi-Anna Kioumourtzoglou. “Principal component pursuit
for pattern identification in environmental mixtures.” Environmental
Health Perspectives 130, no. 11 (2022): 117008.</p>
<p>Zhang, Junhui, Jingkai Yan, and John Wright. “Square root principal
component pursuit: tuning-free noisy robust matrix recovery.” Advances
in Neural Information Processing Systems 34 (2021): 29464-29475.</p>
<p>Boyd, Stephen, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan
Eckstein. “Distributed optimization and statistical learning via the
alternating direction method of multipliers.” Foundations and Trends in
Machine learning 3, no. 1 (2011): 1-122.</p>
<p>Gao, Wenbo, Donald Goldfarb, and Frank E. Curtis. “ADMM for
multiaffine constrained optimization.” Optimization Methods and Software
35, no. 2 (2020): 257-303.</p>
<p>Cherapanamjeri, Yeshwanth, Kartik Gupta, and Prateek Jain. “Nearly
optimal robust matrix completion.” International Conference on Machine
Learning. PMLR, 2017.</p>
<p>Candès, Emmanuel J., Xiaodong Li, Yi Ma, and John Wright. “Robust
principal component analysis?.” Journal of the ACM (JACM) 58, no. 3
(2011): 1-37.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://lawrence-chillrud.github.io/" class="external-link">Lawrence G. Chillrud</a>, <a href="https://www.linkedin.com/in/jaime-benavides-72959356/" class="external-link">Jaime Benavides</a>, <a href="https://lizzy.codes/index.html" class="external-link">Elizabeth A. Gibson</a>, <a href="https://scholar.google.com/citations?user=LyLU8koAAAAJ&amp;hl=en" class="external-link">Junhui Zhang</a>, <a href="https://www.linkedin.com/in/jingkai-yan-a49a9516a/" class="external-link">Jingkai Yan</a>, <a href="https://www.columbia.edu/~jw2966/" class="external-link">John N. Wright</a>, <a href="https://jeffgoldsmith.com/" class="external-link">Jeff Goldsmith</a>, <a href="https://marianthi.github.io/makLAB.github.io/" class="external-link">Marianthi-Anna Kioumourtzoglou</a>, <a href="https://www.publichealth.columbia.edu/" class="external-link"><img src="https://www.publichealth.columbia.edu/sites/default/files/logo-mailman-blue-horizontal.svg?stefxf" width="240" alt=""></a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
