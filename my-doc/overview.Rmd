---
title: "Principal component pursuit via `pcpr`"
subtitle: "Pattern recognition in environmental mixtures analyses"
author: "Lawrence G. Chillrud"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    use_bookdown: true
    toc_depth: 6
---
```{css, echo = FALSE}
#content{
    max-width:1920px;
}
```

```{r options, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.height = 8, message = FALSE)
```

In this document, we illustrate how to employ Principal Component Pursuit (PCP) for pattern recognition in environmental health (EH) analyses with the `pcpr` R package. Additional information regarding `pcpr`'s statistical methodology and computational details are provided in [Chillrud et al. (2022)](). 

# Overview

## The mixtures model

PCP models an observed exposure matrix $D$ as the sum of three underlying ground-truth matrices:
$$
\underset{\text{mixture}}{D_{n \times p}} = \underset{\text{low-rank}}{L_0} + \underset{\text{sparse}}{S_0} + \underset{\text{noise}}{Z_0}
$$
a low-rank matrix $L_0$ encoding consistent patterns of exposure, a sparse matrix $S_0$ isolating unique or outlying exposure events (that cannot be explained by the consistent exposure patterns), and dense noise $Z_0$. All of these matrices are of dimension $n \times p$, where $n$ is the number of observations (e.g., study participants or measurement dates) and $p$ is the number of exposures (chemical and/or non-chemical stressors). Beyond this mixtures model, the main (EH-relevant) assumption made by PCP is that $Z_0 \sim \mathcal{N}(\mu, \sigma^2)$ consists of i.i.d. Gaussian noise corrupting each entry of the overall exposure matrix $D$. A visual example of PCP's mixtures model is provided below:

![A visual example of the PCP mixtures model.](model_example.png)


Under these minimal assumptions, PCP aims to decompose the observed mixture $D$ into the above mixtures model via an optimization program, thereby recovering the ground-truth $L_0$ and $S_0$ with accurate estimates $\hat{L}$ and $\hat{S}$.

## The low-rank matrix

The estimated low-rank matrix $\hat{L}$ provides information on the consistent exposure patterns, satisfying
$$
    r = \text{rank}(\hat{L}) \ll \min(n, p).
$$
The rank $r$ corresponds to the (relatively few) number of underlying patterns governing the mixture, such as specific sources or behaviors leading to exposure. Notice that $\hat{L} \in \mathbb{R}^{n \times p}$, meaning it is still defined in terms of the original variables. Put differently, $\hat{L}$ can be taken as a robust approximation to the true environmental mixture matrix, unperturbed by outliers (captured in $\hat{S}$) or noise (handled by PCP's noise term). In this way, the latent exposure patterns are *encoded* in $\hat{L}$ rather than directly estimated. To explicitly obtain the exposure patterns from $\hat{L}$, PCP may then be paired with various matrix factorization methods (e.g., PCA, factor analysis, or NMF) that yield chemical loadings and individual scores for use in downstream health models.

## The sparse matrix

The estimated sparse matrix $\hat{S}$ captures unusually high/low outlying exposure events, unexplained by the identified patterns in $\hat{L}$. Most entries in $\hat{S}$ are 0, with non-zero entries identifying such extreme exposure activity. The number, location (i.e., support), and value of non-zero entries in $\hat{S}$ need not be \textit{a priori} defined; PCP isolates these during optimization.

# PCP algorithms

|                                     | `root_pcp()`            | `RRMC()`               |
|-------------------------------------|-------------------------|------------------------|
| Convex?                             | _Yes_                   | _No_                   |
| Convergence?                        | _Slow_                  | _Fast_                 |
| Expected low-rank structure?        | _Well-defined_          | _Messy_                |
| Parameters?                         | $D, \lambda, \mu$       | $D, r, \eta$           |
| Supports LOD penalty?               | _Yes_                   | _Yes_                  |
| Supports non-negativity constraint? | _Yes_                   | _No_                   |
| Rank determination?                 | _Autonomous_            | _User-defined_         |
| Solver?                             | _ADMM_                  | _Iterative rank-based_ |

## Root PCP

The convex model available in `pcpr` is Root PCP, denoted here as $\sqrt{\text{PCP}}$. For a comprehensive technical understanding, we refer readers to [Zhang et al. (2021)](https://proceedings.neurips.cc/paper/2021/file/f65854da4622c1f1ad4ffeb361d7703c-Paper.pdf) introducing $\sqrt{\text{PCP}}$. Here we briefly introduce the `root_pcp` function in `pcpr`. 

## RRMC

# Environmental health example analysis: PM2.5 source apportionment

Here we will demonstrate how to apply PCP to environmental mixtures data using an example air pollution source apportionment analysis. Environmental health researchers often aim to identify sources that drive potentially harmful environmental exposures. Investigating potential associations between the identified sources, or patterns of exposure, and adverse health outcomes can then help pave the way toward targeted interventions or public health policy recommendations. 

For our analysis, we aim to apportion speciated PM$_{2.5}$ to its sources using the `queens` dataset that comes with the `pcpr` R package. The `queens` dataset consists of real chemical concentrations (in µg/m$^3$) of 26 species of PM$_{2.5}$ measured every three to six days from 04/04/2001 through 12/30/2021 by an EPA AQS air monitor located in Queens, New York City.

We'll begin by exploring the raw `queens` data - corresponding to the $D$ matrix in our formal mixtures model above - before applying PCP to obtain estimates for $\hat{L}$ and $\hat{S}$. We can then compare the PCP-recovered low dimensional structure in $\hat{L}$ with the observed raw patterns of $D$.

## Exploring the raw `queens` data

First, let's load the `pcpr` package (along with a few other packages we will need for data processing) and take a look at the `queens` dataset:
```{r}
library(dplyr)
library(ggplot2)
library(magrittr)
library(pcpr)
library(stringr)
library(tidyr)
library(lubridate)
library(forcats)

queens 
```

Let's spend some time visualizing some of the trends of the raw dataset, to get a sense of what we're dealing with. We can start by plotting each of the measured chemical species as timeseries. Plotted in black are the raw observed PM$_{2.5}$ measurements (in µg/m$^3$) over time (04/04/2001 - 12/30/2021), and plotted in red are some (rough) lines of best fit: 
```{r}
queens %>%
  pivot_longer(
    colnames(queens)[-1], names_to = "chem", values_to = "concentration"
  ) %>%
  filter(!is.na(concentration)) %>% 
  ggplot(aes(x = Date, y = concentration)) +
  geom_line() +
  geom_smooth(color = "red", formula="y ~ x", method = "loess", span = 0.05) +
  facet_wrap(~chem, scales = "free_y") +
  labs(x = "Date", y = "Concentration (µg/m^3)") +
  theme_bw()
```

Some initial points to make note of, looking at the data:

* The y-axis scale for each chemical's concentration differs widely (e.g., NH4 ranges from 0 to 9 µg/m$^3$, while Ti ranges from 0 to 0.04 µg/m$^3$). We will have to take this into account during preprocessing (which we do in the next section [Preliminary PCA](#prelim-pca)).
* Some chemical species record negative measurements (e.g., Al, Ba, Cd). As explained in the EPA [AQS data documentation](https://aqs.epa.gov/aqsweb/documents/about_aqs_data.html#acceptable-values), this is due to idiosyncrasies in the instruments used to collect the measurements. 
* We can see strong seasonal trends in some species (e.g., NO3), as well as a reduction in concentration over time in others (e.g., Ni).
* Many species have prominent outliers, or extreme exposure events (e.g., Cr, Cu). Further, some species have extreme exposure events following seasonal trends, e.g., K.
* Elemental carbon (EC) and organic carbon (OC) are both missing measurements for the first 8 years of the dataset, likely because the air monitors for those two species were not operational from 2001 - 2009. Handling such systematic missingness is out of scope for this tutorial, so we make the simple decision to omit all data from 2001 - 2009, rather than dropping EC and OC, since including both will be helpful in our source apportionment study.

Below we remove the early measurement dates from 2001 to 2009 that are missing observations for EC and OC, yielding the `queens_small` dataset that we will use moving forward:

LGC 2/4/25 TODO: Justify why we actually begin our analysis in 2015.

```{r}
start_date_df <- queens %>% select(Date, EC, OC) %>% na.omit() %>% slice_head(n = 1)
start_date <- "2015-01-01" # start_date_df$Date[1]
cat("Start date:", as.character(start_date))
queens_small <- queens %>% filter(Date >= as.Date(start_date))
queens_small
```

Next, let's take a look at the correlation structure of our `queens_small` data:
```{r}
queens_small %>% 
  select(-Date) %>%
  as_tibble() %>% 
  GGally::ggcorr(., method = "pairwise.complete.obs", limits = F, label = F, size = 5)
```

The high dimensionality of our `queens_small` air pollution mixture matrix gives rise to this relatively complex correlation matrix. No strong patterns jump out right away here, although Na's correlation with Cl and Mg calls to mind sea salt from the Atlantic Ocean or perhaps road salt kicked up by traffic. We'd like to employ PCP in order to reduce the complexity of our data for more robust downstream analysis. Keep this correlation matrix in mind, since after applying PCP, we'll examine the correlations matrix of $\hat{L}$ and compare.

## Preliminary PCA{#prelim-pca}

Before applying PCP to our mixture, it's good to first take a look at what Principal Component Analysis (PCA) is able to extract from the `queens_small` data. We do this for a couple of reasons: 

1. First, we'd like to establish a baseline for comparison. To better understand the effect of applying PCP to the `queens_small` data, we'd like to see what low dimensional structures exist in the raw data from the outset. 
2. Second, understanding the behavior of the singular values governing the raw data will help inform choices we must make pertaining to PCP's optimization scheme. A preliminary PCA will enable us to scrutinize our data's singular values. $\sqrt{\text{PCP}}$ is well-suited for data exhibiting rapidly decaying singular values (e.g., imaging data), while RRMC is best for more messy data with slowly decaying singular values (environmental mixtures data typically falls into this bucket).

Before we are able to call upon PCA (or PCP, for that matter) we need to first preprocess our data for better numerical stability. In practical terms, both PCA and PCP's statistical routines are sensitive to data with variable scales. Before arbitrarily normalizing or standardizing our data, let's examine the distribution of our `queens_small` dataset:

```{r}
queens_small %>%
  pivot_longer(
    colnames(queens_small)[-1], names_to = "chem", values_to = "concentration"
  ) %>%
  filter(!is.na(concentration)) %>% 
  ggplot(aes(x = concentration)) +
  geom_histogram(bins = 50) +
  theme_bw() +
  facet_wrap(~chem, scales = "free")
```

We have many choices for how we'd like to preprocess our data, e.g., standardize the data, min-max normalization, etc. Because most of our data appears (roughly) normally distributed, we will follow a conventional source apportionment preprocessing approach by scaling (but not centering) each PM2.5 species in our dataset:
```{r}
queens_scaled <- queens_small %>%
  select(-Date) 

queens_scaled[queens_scaled < 0] <- 0

queens_scaled <- queens_scaled %>%
  scale(center = F) %>%
  as_tibble() 

queens_scaled$Date <- queens_small$Date

queens_scaled %>% 
  pivot_longer(
    colnames(queens_small)[-1], names_to = "chem", values_to = "concentration"
  ) %>%
  filter(!is.na(concentration)) %>%
  ggplot(aes(x = concentration)) +
  geom_histogram(bins = 50) +
  theme_bw() +
  facet_wrap(~chem, scales = "free")
```
Great, now we're ready to run the preliminary PCA analysis! We'll use the `stats::prcomp` function for this, which by default omits missing values in our dataset. Instead, let's handle `NA`s by imputing with the column-wise (i.e. per chemical) means. Below, we define two helper functions, `mean_impute` and `pca`:

```{r}
mean_impute <- function(x) {
  x[is.na(x)] <- mean(x, na.rm = T)
  x
}

pca <- function(mat, pcs = c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6"), colgroups = NULL, impute = T) {
  
  if ("Date" %in% colnames(mat)) mat <- mat %>% select(-Date)
  if (impute) mat <- mat %>% apply(., 2, mean_impute)
  
  #### PCA ####
  pca.ln <- prcomp(mat) 
  
  #### VARIANCE TABLE ####
  singvalues_ln <- matrix(pca.ln$sdev^2)
  perc_variance <- round(100*matrix(pca.ln$sdev^2/sum(pca.ln$sdev^2)),1)
  
  pca_summary <- data.frame("Principle component" = 1:min(dim(mat)), "Singular values" = singvalues_ln, "Percent variance" = perc_variance, "Total cumulative variance" = purrr::accumulate(perc_variance, sum))
  
  var_tbl <- kableExtra::kbl(pca_summary, col.names = c("Principle component", "Singular value", "% variance", "Total cumulative variance"), align = "c") %>% 
    kableExtra::kable_classic(full_width = F, html_font = "Cambria", position = "center") %>% 
    kableExtra::kable_styling(bootstrap_options = c("hover", "condensed"), fixed_thead = T)
  
  #### LOADINGS ####
  pca.ln.ld <- as.data.frame.matrix(pca.ln$rotation)
  pca.ln.ld$chem <- row.names(pca.ln.ld)
  
  if (!is.null(colgroups)) {
    colgroups <- colgroups %>% dplyr::rename(chem = !!names(colgroups)[1])
  } else {
    colgroups <- data.frame(chem = colnames(mat), group = "1")
  }
  grouping <- names(colgroups)[2]
  
  plot_loadings_pca <- pca.ln.ld %>% 
    tidyr::gather(key = "PC", value = "Loading", -chem) %>% 
    tibble::as_tibble() %>% 
    dplyr::right_join(., colgroups, by = "chem")
  plot_loadings_pca$chem <- factor(as.character(plot_loadings_pca$chem), levels = unique(as.character(plot_loadings_pca$chem)))
  
  loadings <- plot_loadings_pca %>%
    dplyr::filter(PC %in% pcs) %>% 
    ggplot(aes(x = chem, y = Loading, color = !!sym(grouping))) + 
    geom_point() +
    geom_segment(aes(yend=0, xend = chem)) +
    facet_wrap(~ PC) + theme_bw() +
    theme(legend.position = "bottom", 
          axis.text.x = element_text(angle = 45, hjust = 1),
          strip.background = element_rect(fill = "white"),
          axis.title.x = element_blank(),
          axis.title.y = element_blank()) +
    geom_hline(yintercept = 0, linewidth = 0.2) + 
    ggtitle("Principle Component Loadings")
  
  list(var = var_tbl, var_df = pca_summary, load = loadings)
}

prelim_pca <- pca(queens_scaled)
```

Now let's take a look at our preliminary PCA. We'll start by examining the singular values and the proportion of variance explained by each component:

```{r, fig.height=8, fig.width=12}
prelim_pca$var
```

```{r}
prelim_pca$var_df %>% 
  ggplot(aes(x = Principle.component, y = Singular.values)) +
  geom_point() +
  geom_segment(aes(y = Singular.values, yend = 0, xend = Principle.component)) +
  theme_bw() +
  labs(x = "Principal component", y = "Singular value") +
  ggtitle("Singular values of preliminary PCA")
```
Here is a ncie example of slowly decaying singular values, suggesting we adopt `RRMC` as the PCP function of choice to reduce the dimensionality in our `queens_scaled` dataset.

We can also briefly take a look at the PCA loadings, corresponding to each of the first six principal components (PCs), or first six patterns governing our data, to get an idea of the sources as PCA sees them in the raw data: 

```{r, fig.height=8, fig.width=12}
prelim_pca$load
```
These first six patterns cumulatively comprise roughly 56% of the variance present in `queens_scaled`, so were we to restrict downstream studies to using these six sources, we will have thrown away nearly half of the information encoded in the raw data. Moreover, it is difficult to identify meaningful patterns in these PCs that correspond to real world PM2.5 sources. For example, notice how nearly every chemical species in our mixture loads on the most dominant PC1. This makes source apportionment very difficult, and is common to see when dealing with a complex, multicollinear dataset such as ours.

Now that we have gotten acquainted with the raw data, let's move on to our PCP analysis.

## PCP parameter gridsearch

Given the structure of the singular values in our data (plotted above), we've determined that the non-convex `RRMC` function is the best place to start. This will allow us to interrogate the mixture with solutions of many different ranks (i.e., number of sources/patterns). To start, we need to run a gridsearch to determine the best hyperparameter `eta` to use with RRMC, along with the appropriate rank `r` that RRMC gravitates towards in order to explain the `queens_scaled` data well. Recall that `eta` is a dial controlling the interplay between how structured our recovered low-rank solution is and how sparse our recovered sparse solution is.

LGC 2/4/25 TODO: Refine above paragraph as well as all the sections (how this document is broken up)

This time we avoid imputing the data with the chemical means, since we would like `RRMC` to recover missing values using the low-rank structure present in the data.

```{r}
D <- queens_scaled %>% 
   select(-Date) %>% 
  as.matrix()
```

Due to `RRMC`'s iterative rank-based procedure, we can fix the rank `r` in our gridsearch to be the maximum rank we would like to search up to. Here we've chosen `r=9`, since we do not expect more than 9 distinct sources to govern `queens` PM2.5 data from 2015-2021 (based on prior studies suggesting 5 sources would perhaps be more reasonable). We should now search through `eta`, ensuring we also cover the rough default value obtained via the `get_pcp_defaults` function in `pcpr`.
```{r long search, eval=FALSE, echo=TRUE}
pcp_defaults <- D %>% get_pcp_defaults()
default_eta <- pcp_defaults$eta

grid <- grid_search_cv(
	mat = D,
	pcp_func = RRMC,
	grid = data.frame(eta = c(c(0.005, 0.01), seq(0.05, 0.60, by = 0.05))),
	r = 9,
	runs = 150,
	parallel_approach = "multicore",
	save_as = here::here("my-doc", "eta_search")
)

```

This takes quite a while, so we have saved the results to a `.rds` file using `grid_search_cv`'s `save_as` argument, which we load below:

```{r}
grid <- readRDS(here::here("my-doc", "11-11-24_huge_eta_search.rds"))
grid$all_stats %>% 
  group_by(r, eta) %>% 
  summarize(
    avg_rel_err = mean(rel_err), 
    avg_sparsity = mean(S_sparsity), 
    avg_rank = mean(L_rank)
  ) %>% 
  arrange(avg_rel_err)
```

The results from the search suggest a rank 4 solution with an `eta` of 0.1 is most faithful to the original data, while also yielding a sparse solution with a sparsity of 99.3%, meaning 0.7% of values in the data were flagged as extreme outlying exposure events. So, we will use these parameters moving forward.

```{r}
plot_mat <- function(D) {
  heatmaply::heatmaply(D, Rowv=F, Colv=F, showticklabels = FALSE, showtickmarks = FALSE)
}
```

```{r}
rrmc_out <- RRMC(D, r = 4, eta = 0.1) # eta = 0.01?
# rachels_pcp <- root_pcp_noncvx_nonnegL_na(D, lambda = 0.00008, mu = 0.005, r = 7)
# my_pcp <- new_root_pcp_noncvx_nonnegL_na_lod(D, lambda = 1e-04, mu = 0.005, r = 5)
```

```{r}
# plot_mat(rrmc_out$L)
plot_mat(rrmc_out$S)
# plot_mat(rachels_pcp$L)
# plot_mat(rachels_pcp$S)
# plot_mat(my_pcp$L)
# plot_mat(my_pcp$S)
```

```{r}
# colnames(rrmc_out$L) <- colnames(D)
# rrmc_pca <- pca(rrmc_out$L, pcs = paste("PC", 1:3, sep = ""))
# rrmc_pca$var

pcp <- rrmc_out
rank <- Matrix::rankMatrix(pcp$L, tol=1e-05)[1]

colnames(pcp$L) <- colnames(D)
pcp_pca <- pca(pcp$L, pcs = paste("PC", 1:rank, sep = ""))
pcp_pca$var
```

```{r}
pcp_pca$load
```

```{r}
print_patterns <- function(pats, colgroups = NULL, n = 1:rank, pat_type = "pat", title = "") {
  
  if (!is.null(colgroups)) {
    colgroups <- colgroups %>% dplyr::rename(chem = !!names(colgroups)[1])
  } else {
    colgroups <- data.frame(chem = rownames(pats), group = "1")
  }
  
  grouping <- names(colgroups)[2]
  
  colnames(pats) <- paste0(pat_type, stringr::str_pad(1:ncol(pats), width = 2, pad = "0", side = "left"))
  
  pats.df <- pats %>% 
    tibble::as_tibble() %>% 
    dplyr::mutate(chem = colgroups[[1]]) %>%
    tidyr::pivot_longer(-chem, names_to = "pattern", values_to = "loading") %>%
    dplyr::right_join(., colgroups, by = "chem")

  pats.df$chem <- factor(as.character(pats.df$chem), levels = unique(as.character(pats.df$chem)))

  loadings <- pats.df %>%
    dplyr::filter(pattern %in% paste0(pat_type, stringr::str_pad(n, width = 2, pad = "0", side = "left"))) %>%
    ggplot(aes(x = chem, y = loading, color = !!sym(grouping))) +
    geom_point() +
    geom_segment(aes(yend=0, xend = chem)) +
    facet_wrap(~ pattern) + theme_bw() +
    theme(legend.position = "bottom",
          axis.text.x = element_text(angle = 45, hjust = 1),
          strip.background = element_rect(fill = "white"),
          axis.title.x = element_blank(),
          axis.title.y = element_blank()) +
    geom_hline(yintercept = 0, size = 0.2) + ggtitle(title)
  
  loadings
}
```

```{r NMF}
library(NMF)

num_cores <- ceiling(parallel::detectCores() / 2)
nmf_mat <- pcp$L
nmf_mat[nmf_mat < 0] <- 0
res <- nmf(nmf_mat, rank = rank, method = "offset", nrun = 30, seed = 0, .opt = paste0('vp', num_cores))

# only compute the scores
s <- featureScore(res)
summary(s)
# compute the scores and characterize each metagene
s <- extractFeatures(res) 
str(s)

W <- basis(res) # basis matrix / metagenes / contribution matrix
H <- coef(res) # mixture coeffecient matrix / metagene expression profiles / profile matrix ie. loadings ?

# heatmaply(W, main = "NMF basis / contribution matrix (scores)", Rowv = F, Colv = F, 
#           ylab = params$rowvar_name, labRow = as.character(rowlabs),
#           cexRow = 100, row_side_colors = data.frame("cohort" = cohorts), showticklabels = c(T, F))
# 
# heatmaply(H, main = "NMF profile matrix (loadings)", Rowv = F, Colv = F,
#           col_side_colors = data.frame("exposure family" = as.factor(params$colgroupings)))

loadings <- t(H) %>% as_tibble()
rownames(loadings) <- colnames(pcp$L)
print_patterns(loadings, title = "NMF loadings")
```

```{r NMF analysis}
nmf_scores <- W %>% 
  as.data.frame() %>% 
  mutate(date = queens_scaled$Date) %>% 
  mutate(DayofWeek = as.character(wday(date, label = TRUE))) %>% 
  mutate(DayofWeek = factor(DayofWeek, levels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"))) %>% 
  as_tibble()

nmf_sources <- nmf_scores %>% pivot_longer(V1:V4, names_to = "source", values_to = "score") %>% 
  group_by(source) %>% 
  mutate(
    source = as_factor(source),
    # source = fct_recode(source, "Crustal" = "V1", "Industrial/Road" = "V2", "Traffic" = "V3", "Regional" = "V4", "Salt" = "V5")
  )

yr_num <- length(unique(year(nmf_sources$date)))
```

```{r trends over time}
ggplot(nmf_sources, aes(date, score)) +
  geom_smooth(method = "loess", span = 0.05, color = "black") +
  facet_wrap(~source, scales = "free", ncol = 1) + 
  xlab("") +
  ylab("Factor Score") +
  theme_classic() +
  theme(
    axis.title.y = element_text(size = 18),
    strip.text = element_text(size = 14),
    axis.text.x = element_text(size = 13),
    axis.text.y = element_text(size = 11)
  ) +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year")
```

```{r seasonal trends}
season <- nmf_sources %>% mutate(season = metR::season(date))
season %>% group_by(season, source) %>% summarise(mean = mean(score)) %>% 
  pivot_wider(names_from = season, values_from = mean)

season %>% group_by(season, source) %>% summarise(sd = sd(score)) %>% 
  pivot_wider(names_from = season, values_from = sd)
```

```{r weekly trends}
week <- nmf_sources %>% 
  group_by(source, DayofWeek) %>% 
  summarize(mean_score = mean(score, na.rm = TRUE))

# plot weekday trends - Figure S5
ggplot(week, aes(DayofWeek, mean_score)) +
  geom_bar(stat = "identity") +
  xlab("") +
  facet_wrap(~source, scales = "free", ncol =1) +
  ylab("Mean Factor Score") +
  ylim(0, 3) +
  theme_classic() +
  theme(
    axis.title.y = element_text(size = 18),
    axis.text.x.bottom = element_text(size = 13, color = "black"),
    strip.text = element_text(size = 14),
    axis.text.y.left = element_text(size = 12)
  )

# tag weekend vs. weekdays
week_day <- nmf_sources %>% 
  mutate(week_end = if_else(DayofWeek %in% 
                              c('Mon', 'Tue', 'Wed', 'Thu', 'Fri'), 
                            'weekday', 'weekend'),
         week_end = as_factor(week_end)) %>% 
  pivot_wider(names_from = source, values_from = score)

# t-test for weekday vs. weekend means - Table S3
# repeat for each source
# t.test(week_day$Salt~week_day$week_end)

# find mean and SD for weekend/weekday
# repeat for each source
week_day %>% group_by(week_end) %>% summarize(m = mean(V1, na.rm = TRUE),
                                              sd = sd(V1, na.rm = TRUE))
```

```{r NMF percentages}
pcp_nmf_source_1 <- nmf_scores %>% 
  select(V1) %>% 
  as.matrix() %>% 
  norm()

pcp_nmf_source_2 <- nmf_scores %>% 
  select(V2) %>% 
  as.matrix() %>% 
  norm()

pcp_nmf_source_3 <- nmf_scores %>% 
  select(V3) %>% 
  as.matrix() %>% 
  norm()

pcp_nmf_source_4 <- nmf_scores %>% 
  select(V4) %>% 
  as.matrix() %>% 
  norm()

# pcp_nmf_source_5 <- nmf_scores %>% 
#   select(V5) %>% 
#   as.matrix() %>% 
#   norm()

# add norms together to get total
tot = pcp_nmf_source_1 + pcp_nmf_source_2 + pcp_nmf_source_3 + pcp_nmf_source_4 #+ pcp_nmf_source_5

# calculate percentages
vals <- c((pcp_nmf_source_1/tot)*100, (pcp_nmf_source_2/tot)*100, (pcp_nmf_source_3/tot)*100, (pcp_nmf_source_4/tot)*100)#, (pcp_nmf_source_5/tot)*100)

#create vector of source names
val_names <- paste0('V', 1:4) # c("Crustal", "Industrial/Road", "Traffic", "Regional", "Salt")

# label percentages vector with source names
names(vals) <- val_names

# make into tibble
values <- cbind(vals, val_names) %>% 
  as_tibble() %>% 
  mutate(vals = as.double(vals),
         vals = round(vals, digits = 0),
         val_names = fct_reorder(val_names, vals)
         )

# pie chart - Figure S4
values %>% 
  as_tibble() %>% 
  mutate(vals = as.double(vals)) %>% 
  ggplot(aes(x = "", y = vals, fill = val_names)) +
    geom_bar(stat = "identity", width = 1) +
    coord_polar("y", start = 0) +
    theme_void() +
    theme(
      legend.title = element_blank(),
      legend.key.size = unit(1, 'cm'),
      legend.text = element_text(size = 14)
      ) +
    geom_text(aes(label = vals),
              position = position_stack(vjust = 0.5),
              size = 6)

```
```{r nmf corr}
nmf_scores %>% 
  # rename(Crustal := V1, Industrial := V2, Traffic := V3, Regional := V4, Salt := V5) %>% 
  GGally::ggcorr(method = c("pairwise.complete.obs", "pearson"), limits = FALSE,
       label = TRUE, label_size = 5,
       hjust = 1, size = 6, color = "grey50", layout.exp = 4)
```
